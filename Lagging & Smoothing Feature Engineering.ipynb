{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#General imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import re\n",
    "\n",
    "import os, sys, gc, time, warnings, pickle, psutil, random\n",
    "\n",
    "from sklearn import preprocessing, metrics\n",
    "import gc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 'sales'         # Our main target\n",
    "END_TRAIN = 1913         # Last day in train set\n",
    "MAIN_INDEX = ['id','d']  # We can identify item by these columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('sales_train_validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ca = train[train['state_id']=='CA']#break up by state in order to compute faster\n",
    "train_tx = train[train['state_id']=='TX']\n",
    "train_wi = train[train['state_id']=='WI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_columns = ['id','item_id','dept_id','cat_id','store_id','state_id']\n",
    "train_ca = pd.melt(train_ca, \n",
    "                  id_vars = index_columns, \n",
    "                  var_name = 'd', \n",
    "                  value_name = TARGET)\n",
    "train_tx = pd.melt(train_tx, \n",
    "                  id_vars = index_columns, \n",
    "                  var_name = 'd', \n",
    "                  value_name = TARGET)\n",
    "train_wi = pd.melt(train_wi, \n",
    "                  id_vars = index_columns, \n",
    "                  var_name = 'd', \n",
    "                  value_name = TARGET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create some temporary df's for the purposes of engineering lag features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Given our size of our data, it might be wise to record how long each function takes so we know where\n",
    "our bottlenecks are.'''\n",
    "\n",
    "from functools import wraps\n",
    "\n",
    "def timer(func):\n",
    "    \"\"\"A decorator that prints how long a function took to run.\"\"\"\n",
    "\n",
    "    # Define the wrapper function to return.\n",
    "    @wraps #preserve the metadata of our function.\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # When wrapper() is called, get the current time.\n",
    "        t_start = time.time()\n",
    "\n",
    "        # Call the decorated function and store the result.\n",
    "        result = func(*args, **kwargs)\n",
    "\n",
    "        # Get the total time it took to run, and print it.\n",
    "        t_total = time.time() - t_start\n",
    "\n",
    "        print('{} took {}s'.format(func.__name__, t_total))        \n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.62 min: Time for bulk shift\n"
     ]
    }
   ],
   "source": [
    "'''Since our data is already sorted by 'd' values, we can easily shift() values\n",
    "as we as aggregate values.'''\n",
    "\n",
    "lags = [col for col in range(15,36,2)]\n",
    "\n",
    "start_time = time.time()\n",
    "ca_shifts = train_ca[['id','d',TARGET]]\n",
    "ca_shifts = ca_shifts.assign(**{\n",
    "        '{}_lag_{}'.format(col, l): ca_shifts.groupby(['id'])[col].transform(lambda x: x.shift(l))\n",
    "        for l in lags\n",
    "        for col in [TARGET]\n",
    "    })\n",
    "\n",
    "print('%0.2f min: Time for bulk shift' % ((time.time() - start_time) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_shifts = train_tx[['id','d',TARGET]]\n",
    "tx_shifts = tx_shifts.assign(**{\n",
    "        '{}_lag_{}'.format(col, l): tx_shifts.groupby(['id'])[col].transform(lambda x: x.shift(l))\n",
    "        for l in lags\n",
    "        for col in [TARGET]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wi_shifts = train_wi[['id','d',TARGET]]\n",
    "wi_shifts = wi_shifts.assign(**{\n",
    "        '{}_lag_{}'.format(col, l): wi_shifts.groupby(['id'])[col].transform(lambda x: x.shift(l))\n",
    "        for l in lags\n",
    "        for col in [TARGET]\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we generated lag features, we can focus on creating smoothing/rolling features. Afterwards, we can focus on dealing with the NaNs created from these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rolling period: 14\n",
      "Rolling period: 30\n",
      "Rolling period: 60\n",
      "Rolling period: 90\n",
      "Rolling period: 180\n"
     ]
    }
   ],
   "source": [
    "'''Rolling averages with different time frames'''\n",
    "\n",
    "ca_rolls = train_ca[['id','d','sales']]\n",
    "\n",
    "for i in [14,30,60, 90,180]:\n",
    "    print('Rolling period:', i)\n",
    "    ca_rolls['rolling_mean_'+str(i)] = ca_rolls.groupby(['id'])[TARGET].transform(lambda x: x.shift(1).rolling(i).mean())\n",
    "    ca_rolls['rolling_std_'+str(i)]  = ca_rolls.groupby(['id'])[TARGET].transform(lambda x: x.shift(1).rolling(i).std())\n",
    "    ca_rolls['rolling_max_'+str(i)]  = ca_rolls.groupby(['id'])[TARGET].transform(lambda x: x.shift(1).rolling(i).max())\n",
    "    ca_rolls['rolling_min_'+str(i)]  = ca_rolls.groupby(['id'])[TARGET].transform(lambda x: x.shift(1).rolling(i).min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rolling period: 14\n",
      "Rolling period: 30\n",
      "Rolling period: 60\n",
      "Rolling period: 90\n",
      "Rolling period: 180\n"
     ]
    }
   ],
   "source": [
    "'''Rolling averages with different time frames'''\n",
    "\n",
    "tx_rolls = train_tx[['id','d','sales']]\n",
    "\n",
    "for i in [14,30,60, 90,180]:\n",
    "    print('Rolling period:', i)\n",
    "    tx_rolls['rolling_mean_'+str(i)] = tx_rolls.groupby(['id'])[TARGET].transform(lambda x: x.shift(1).rolling(i).mean())\n",
    "    tx_rolls['rolling_std_'+str(i)]  = tx_rolls.groupby(['id'])[TARGET].transform(lambda x: x.shift(1).rolling(i).std())\n",
    "    tx_rolls['rolling_max_'+str(i)]  = tx_rolls.groupby(['id'])[TARGET].transform(lambda x: x.shift(1).rolling(i).max())\n",
    "    tx_rolls['rolling_min_'+str(i)]  = tx_rolls.groupby(['id'])[TARGET].transform(lambda x: x.shift(1).rolling(i).min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rolling period: 14\n",
      "Rolling period: 30\n",
      "Rolling period: 60\n",
      "Rolling period: 90\n",
      "Rolling period: 180\n"
     ]
    }
   ],
   "source": [
    "'''Rolling averages with different time frames'''\n",
    "\n",
    "wi_rolls = train_wi[['id','d','sales']]\n",
    "\n",
    "for i in [14,30,60, 90,180]:\n",
    "    print('Rolling period:', i)\n",
    "    wi_rolls['rolling_mean_'+str(i)] = wi_rolls.groupby(['id'])[TARGET].transform(lambda x: x.shift(1).rolling(i).mean())\n",
    "    wi_rolls['rolling_std_'+str(i)]  = wi_rolls.groupby(['id'])[TARGET].transform(lambda x: x.shift(1).rolling(i).std())\n",
    "    wi_rolls['rolling_max_'+str(i)]  = wi_rolls.groupby(['id'])[TARGET].transform(lambda x: x.shift(1).rolling(i).max())\n",
    "    wi_rolls['rolling_min_'+str(i)]  = wi_rolls.groupby(['id'])[TARGET].transform(lambda x: x.shift(1).rolling(i).min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created lagged and smoothing features, to finish our feature engineering we can create features based on day - whether its a holiday, weekend, etc.\n",
    "\n",
    "Later, we will label encode these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal = pd.read_csv('calendar.csv', parse_dates = ['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_list=[i for i in cal.event_name_1.fillna(0).unique() if i != 0] \n",
    "\n",
    "#Extract all the days an event has in the span of 1916 days\n",
    "day_event_list=[cal[cal.event_name_1==i].d.tolist() for i in event_list]\n",
    "\n",
    "#Create the Event_df dataframe which we will use throughout the notebook\n",
    "event_df=pd.DataFrame({'Event Name' : event_list, 'Event day':day_event_list})\n",
    "restricted_day= set(['d_'+ str(i) for i in np.arange(1916,1970)])\n",
    "quantity=[]\n",
    "\n",
    "for i in day_event_list:\n",
    "    # Making sure that we exclude all the days thats are not in the training set\n",
    "    clean_i=list(set(i)-restricted_day)\n",
    "    temp=train[clean_i].sum().sum() #Adding columns and then rows\n",
    "    quantity.append(temp)\n",
    "\n",
    "event_df['Quantity']=quantity\n",
    "\n",
    "all_events = event_df['Event day'].values\n",
    "all_events = np.concatenate(all_events, axis=0)\n",
    "all_events = all_events.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['d_9', 'd_373', 'd_737', 'd_1101', 'd_1465', 'd_1836', 'd_17',\n",
       "       'd_382', 'd_748', 'd_1113', 'd_1478', 'd_1843', 'd_24', 'd_388',\n",
       "       'd_752', 'd_1116', 'd_1480', 'd_1844', 'd_40', 'd_390', 'd_747',\n",
       "       'd_1132', 'd_1482', 'd_1839', 'd_47', 'd_397', 'd_754', 'd_1139',\n",
       "       'd_1489', 'd_1846', 'd_48', 'd_414', 'd_779', 'd_1144', 'd_1509',\n",
       "       'd_1875', 'd_51', 'd_405', 'd_758', 'd_1143', 'd_1497', 'd_1882',\n",
       "       'd_86', 'd_443', 'd_828', 'd_1535', 'd_1920', 'd_88', 'd_442',\n",
       "       'd_795', 'd_1180', 'd_1534', 'd_1919', 'd_97', 'd_463', 'd_1193',\n",
       "       'd_1558', 'd_1924', 'd_100', 'd_471', 'd_835', 'd_1199', 'd_1563',\n",
       "       'd_1927', 'd_122', 'd_486', 'd_850', 'd_1214', 'd_1578', 'd_1949',\n",
       "       'd_123', 'd_501', 'd_860', 'd_1224', 'd_1588', 'd_1952', 'd_135',\n",
       "       'd_510', 'd_874', 'd_1234', 'd_1600', 'd_1969', 'd_142', 'd_506',\n",
       "       'd_870', 'd_1605', 'd_157', 'd_523', 'd_888', 'd_1253', 'd_1618',\n",
       "       'd_185', 'd_539', 'd_893', 'd_1248', 'd_1602', 'd_1957', 'd_215',\n",
       "       'd_569', 'd_923', 'd_1278', 'd_1632', 'd_220', 'd_584', 'd_948',\n",
       "       'd_1312', 'd_1683', 'd_255', 'd_619', 'd_990', 'd_1354', 'd_1718',\n",
       "       'd_276', 'd_642', 'd_1007', 'd_1372', 'd_1737', 'd_283', 'd_637',\n",
       "       'd_991', 'd_1345', 'd_1700', 'd_287', 'd_653', 'd_1018', 'd_1383',\n",
       "       'd_1748', 'd_300', 'd_664', 'd_1035', 'd_1399', 'd_1763', 'd_331',\n",
       "       'd_697', 'd_1062', 'd_1427', 'd_1792', 'd_334', 'd_688', 'd_1042',\n",
       "       'd_1426', 'd_1781', 'd_338', 'd_704', 'd_1069', 'd_1434', 'd_1799',\n",
       "       'd_344', 'd_710', 'd_1075', 'd_1440', 'd_1805', 'd_353', 'd_724',\n",
       "       'd_1088', 'd_1452', 'd_1816', 'd_436', 'd_793', 'd_1178', 'd_1528',\n",
       "       'd_1885'], dtype='<U6')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca['event'] = ca['d'].apply(lambda x: 1 if x in all_events else 0)\n",
    "tx['event'] = tx['d'].apply(lambda x: 1 if x in all_events else 0)\n",
    "wi['event'] = wi['d'].apply(lambda x: 1 if x in all_events else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>d</th>\n",
       "      <th>bollinger</th>\n",
       "      <th>rrg_bench</th>\n",
       "      <th>rrg_item</th>\n",
       "      <th>ATR</th>\n",
       "      <th>ann_vol</th>\n",
       "      <th>entropy</th>\n",
       "      <th>beta</th>\n",
       "      <th>info_ratio</th>\n",
       "      <th>triple_exp</th>\n",
       "      <th>relative_vol</th>\n",
       "      <th>RS</th>\n",
       "      <th>gap_size</th>\n",
       "      <th>gap_cat</th>\n",
       "      <th>gap_start</th>\n",
       "      <th>number_of_gaps</th>\n",
       "      <th>rolling_max_14</th>\n",
       "      <th>rolling_max_180</th>\n",
       "      <th>rolling_max_30</th>\n",
       "      <th>rolling_max_60</th>\n",
       "      <th>rolling_max_90</th>\n",
       "      <th>rolling_mean_14</th>\n",
       "      <th>rolling_mean_180</th>\n",
       "      <th>rolling_mean_30</th>\n",
       "      <th>rolling_mean_60</th>\n",
       "      <th>rolling_mean_90</th>\n",
       "      <th>rolling_min_14</th>\n",
       "      <th>rolling_min_180</th>\n",
       "      <th>rolling_min_30</th>\n",
       "      <th>rolling_min_60</th>\n",
       "      <th>rolling_min_90</th>\n",
       "      <th>rolling_std_14</th>\n",
       "      <th>rolling_std_180</th>\n",
       "      <th>rolling_std_30</th>\n",
       "      <th>rolling_std_60</th>\n",
       "      <th>rolling_std_90</th>\n",
       "      <th>sales_lag_15</th>\n",
       "      <th>sales_lag_17</th>\n",
       "      <th>sales_lag_19</th>\n",
       "      <th>sales_lag_21</th>\n",
       "      <th>sales_lag_23</th>\n",
       "      <th>sales_lag_25</th>\n",
       "      <th>sales_lag_27</th>\n",
       "      <th>sales_lag_29</th>\n",
       "      <th>sales_lag_31</th>\n",
       "      <th>sales_lag_33</th>\n",
       "      <th>sales_lag_35</th>\n",
       "      <th>event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_001_CA_1_validation</td>\n",
       "      <td>d_1201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>18.234375</td>\n",
       "      <td>1.646484</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>921.0</td>\n",
       "      <td>beginning</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.214233</td>\n",
       "      <td>0.427734</td>\n",
       "      <td>0.300049</td>\n",
       "      <td>0.350098</td>\n",
       "      <td>0.366699</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.425781</td>\n",
       "      <td>0.660156</td>\n",
       "      <td>0.535156</td>\n",
       "      <td>0.546875</td>\n",
       "      <td>0.589355</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_002_CA_1_validation</td>\n",
       "      <td>d_1201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>18.234375</td>\n",
       "      <td>1.111328</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.001097</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>921.0</td>\n",
       "      <td>beginning</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.214233</td>\n",
       "      <td>0.166626</td>\n",
       "      <td>0.266602</td>\n",
       "      <td>0.199951</td>\n",
       "      <td>0.166626</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.579102</td>\n",
       "      <td>0.416260</td>\n",
       "      <td>0.520996</td>\n",
       "      <td>0.443359</td>\n",
       "      <td>0.403564</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HOBBIES_1_003_CA_1_validation</td>\n",
       "      <td>d_1201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>18.234375</td>\n",
       "      <td>1.176758</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000176</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>921.0</td>\n",
       "      <td>beginning</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.285645</td>\n",
       "      <td>0.194458</td>\n",
       "      <td>0.233276</td>\n",
       "      <td>0.350098</td>\n",
       "      <td>0.333252</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.611328</td>\n",
       "      <td>0.529785</td>\n",
       "      <td>0.503906</td>\n",
       "      <td>0.659180</td>\n",
       "      <td>0.618164</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HOBBIES_1_004_CA_1_validation</td>\n",
       "      <td>d_1201</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>18.234375</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041687</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>921.0</td>\n",
       "      <td>beginning</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.339844</td>\n",
       "      <td>2.267578</td>\n",
       "      <td>2.349609</td>\n",
       "      <td>2.388672</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.414062</td>\n",
       "      <td>2.417969</td>\n",
       "      <td>2.449219</td>\n",
       "      <td>2.433594</td>\n",
       "      <td>2.607422</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HOBBIES_1_005_CA_1_validation</td>\n",
       "      <td>d_1201</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>18.234375</td>\n",
       "      <td>2.099609</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023849</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>921.0</td>\n",
       "      <td>beginning</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.643066</td>\n",
       "      <td>0.833496</td>\n",
       "      <td>0.799805</td>\n",
       "      <td>0.799805</td>\n",
       "      <td>0.799805</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.928711</td>\n",
       "      <td>1.027344</td>\n",
       "      <td>0.924805</td>\n",
       "      <td>0.935059</td>\n",
       "      <td>0.901855</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id       d  bollinger  rrg_bench  rrg_item  ATR  \\\n",
       "1  HOBBIES_1_001_CA_1_validation  d_1201        0.0        5.0       5.0  3.0   \n",
       "3  HOBBIES_1_002_CA_1_validation  d_1201        0.0        5.0       5.0  3.0   \n",
       "5  HOBBIES_1_003_CA_1_validation  d_1201        0.0        5.0       5.0  3.0   \n",
       "7  HOBBIES_1_004_CA_1_validation  d_1201        2.0        5.0       5.0  3.0   \n",
       "9  HOBBIES_1_005_CA_1_validation  d_1201        2.0        5.0       5.0  3.0   \n",
       "\n",
       "     ann_vol   entropy  beta  info_ratio  triple_exp  relative_vol   RS  \\\n",
       "1  18.234375  1.646484   0.0    0.010735         0.0           0.0  0.0   \n",
       "3  18.234375  1.111328   0.0   -0.001097         0.0           0.0  0.0   \n",
       "5  18.234375  1.176758   0.0   -0.000176         0.0           0.0  0.0   \n",
       "7  18.234375  2.812500   0.0    0.041687         2.0           0.0  0.0   \n",
       "9  18.234375  2.099609   0.0    0.023849         2.0           0.0  0.0   \n",
       "\n",
       "   gap_size    gap_cat  gap_start  number_of_gaps  rolling_max_14  \\\n",
       "1     921.0  beginning        4.0             1.0             1.0   \n",
       "3     921.0  beginning        4.0             1.0             2.0   \n",
       "5     921.0  beginning        4.0             1.0             2.0   \n",
       "7     921.0  beginning        4.0             1.0            14.0   \n",
       "9     921.0  beginning        4.0             1.0             3.0   \n",
       "\n",
       "   rolling_max_180  rolling_max_30  rolling_max_60  rolling_max_90  \\\n",
       "1              3.0             2.0             2.0             2.0   \n",
       "3              2.0             2.0             2.0             2.0   \n",
       "5              3.0             2.0             3.0             3.0   \n",
       "7             14.0            14.0            14.0            14.0   \n",
       "9              5.0             3.0             3.0             3.0   \n",
       "\n",
       "   rolling_mean_14  rolling_mean_180  rolling_mean_30  rolling_mean_60  \\\n",
       "1         0.214233          0.427734         0.300049         0.350098   \n",
       "3         0.214233          0.166626         0.266602         0.199951   \n",
       "5         0.285645          0.194458         0.233276         0.350098   \n",
       "7         2.500000          2.339844         2.267578         2.349609   \n",
       "9         0.643066          0.833496         0.799805         0.799805   \n",
       "\n",
       "   rolling_mean_90  rolling_min_14  rolling_min_180  rolling_min_30  \\\n",
       "1         0.366699             0.0              0.0             0.0   \n",
       "3         0.166626             0.0              0.0             0.0   \n",
       "5         0.333252             0.0              0.0             0.0   \n",
       "7         2.388672             0.0              0.0             0.0   \n",
       "9         0.799805             0.0              0.0             0.0   \n",
       "\n",
       "   rolling_min_60  rolling_min_90  rolling_std_14  rolling_std_180  \\\n",
       "1             0.0             0.0        0.425781         0.660156   \n",
       "3             0.0             0.0        0.579102         0.416260   \n",
       "5             0.0             0.0        0.611328         0.529785   \n",
       "7             0.0             0.0        3.414062         2.417969   \n",
       "9             0.0             0.0        0.928711         1.027344   \n",
       "\n",
       "   rolling_std_30  rolling_std_60  rolling_std_90  sales_lag_15  sales_lag_17  \\\n",
       "1        0.535156        0.546875        0.589355           0.0           0.0   \n",
       "3        0.520996        0.443359        0.403564           1.0           0.0   \n",
       "5        0.503906        0.659180        0.618164           0.0           0.0   \n",
       "7        2.449219        2.433594        2.607422           1.0           1.0   \n",
       "9        0.924805        0.935059        0.901855           2.0           0.0   \n",
       "\n",
       "   sales_lag_19  sales_lag_21  sales_lag_23  sales_lag_25  sales_lag_27  \\\n",
       "1           0.0           0.0           0.0           1.0           0.0   \n",
       "3           0.0           1.0           0.0           0.0           0.0   \n",
       "5           1.0           0.0           0.0           0.0           0.0   \n",
       "7           2.0           4.0           4.0           3.0           2.0   \n",
       "9           1.0           0.0           3.0           0.0           1.0   \n",
       "\n",
       "   sales_lag_29  sales_lag_31  sales_lag_33  sales_lag_35  event  \n",
       "1           0.0           0.0           0.0           1.0      0  \n",
       "3           0.0           0.0           1.0           0.0      0  \n",
       "5           0.0           1.0           0.0           0.0      0  \n",
       "7           1.0           3.0           2.0           0.0      0  \n",
       "9           2.0           0.0           0.0           1.0      0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing our categoricals to numeric as we prepare for modeling.\n",
    "ca.loc[ca['gap_cat']=='beginning', 'gap_placement'] = 0\n",
    "ca.loc[ca['gap_cat']=='middle', 'gap_placement'] = 1\n",
    "ca.loc[ca['gap_cat']=='end', 'gap_placement'] = 2\n",
    "\n",
    "\n",
    "tx.loc[tx['gap_cat']=='beginning', 'gap_placement'] = 0\n",
    "tx.loc[tx['gap_cat']=='middle', 'gap_placement'] = 1\n",
    "tx.loc[tx['gap_cat']=='end', 'gap_placement'] = 2\n",
    "\n",
    "wi.loc[wi['gap_cat']=='beginning', 'gap_placement'] = 0\n",
    "wi.loc[wi['gap_cat']=='middle', 'gap_placement'] = 1\n",
    "wi.loc[wi['gap_cat']=='end', 'gap_placement'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca.drop('gap_cat', axis=1, inplace=True)\n",
    "tx.drop('gap_cat', axis=1, inplace=True)\n",
    "wi.drop('gap_cat', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca.to_pickle('ca_final.pkl')\n",
    "tx.to_pickle('tx_final.pkl')\n",
    "wi.to_pickle('wi_final.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 208.77 MB\n",
      "Memory usage after optimization is: 45.77 MB\n",
      "Decreased by 78.1%\n",
      "Sell prices has 6841121 rows and 4 columns\n",
      "Memory usage of dataframe is 0.21 MB\n",
      "Memory usage after optimization is: 0.22 MB\n",
      "Decreased by -5.7%\n",
      "Calendar has 1969 rows and 14 columns\n",
      "Memory usage of dataframe is 446.40 MB\n",
      "Memory usage after optimization is: 95.42 MB\n",
      "Decreased by 78.6%\n",
      "Sales train validation has 30490 rows and 1919 columns\n"
     ]
    }
   ],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object and col_type.name != 'category' and 'datetime' not in col_type.name:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        elif 'datetime' not in col_type.name:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "   \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "sell_prices_df = pd.read_csv('sell_prices.csv')\n",
    "sell_prices_df = reduce_mem_usage(sell_prices_df)\n",
    "print('Sell prices has {} rows and {} columns'.format(sell_prices_df.shape[0], sell_prices_df.shape[1]))\n",
    "\n",
    "calendar_df = pd.read_csv('calendar.csv')\n",
    "calendar_df = reduce_mem_usage(calendar_df)\n",
    "print('Calendar has {} rows and {} columns'.format(calendar_df.shape[0], calendar_df.shape[1]))\n",
    "\n",
    "sales_train_validation_df = pd.read_csv('sales_train_validation.csv')\n",
    "sales_train_validation_df = reduce_mem_usage(sales_train_validation_df)\n",
    "print('Sales train validation has {} rows and {} columns'.format(sales_train_validation_df.shape[0], sales_train_validation_df.shape[1]))\n",
    "\n",
    "submission_df = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_ITEMS = sales_train_validation_df.shape[0]  # 30490\n",
    "DAYS_PRED = 28\n",
    "nrows = 365 * 2 * NUM_ITEMS\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 0.27 MB\n",
      "Memory usage after optimization is: 0.23 MB\n",
      "Decreased by 16.5%\n",
      "Memory usage of dataframe is 95.72 MB\n",
      "Memory usage after optimization is: 95.32 MB\n",
      "Decreased by 0.4%\n",
      "Memory usage of dataframe is 78.29 MB\n",
      "Memory usage after optimization is: 45.67 MB\n",
      "Decreased by 41.7%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def encode_categorical(df, cols):\n",
    "    for col in cols:\n",
    "        # Leave NaN as it is.\n",
    "        le = LabelEncoder()\n",
    "        not_null = df[col][df[col].notnull()]\n",
    "        df[col] = pd.Series(le.fit_transform(not_null), index=not_null.index)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "calendar_df = encode_categorical(calendar_df, [\"event_name_1\", \"event_type_1\", \"event_name_2\", \"event_type_2\"]).pipe(reduce_mem_usage)\n",
    "sales_train_validation_df = encode_categorical(sales_train_validation_df, [\"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]).pipe(reduce_mem_usage)\n",
    "sell_prices_df = encode_categorical(sell_prices_df, [\"item_id\", \"store_id\"]).pipe(reduce_mem_usage)    \n",
    "    \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def melt_and_merge(calendar, sell_prices, sales_train_validation, submission, nrows = 55000000, merge = False):\n",
    "    \n",
    "    # melt sales data, get it ready for training\n",
    "    sales_train_validation = pd.melt(sales_train_validation, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day', value_name = 'demand')\n",
    "    print('Melted sales train validation has {} rows and {} columns'.format(sales_train_validation.shape[0], sales_train_validation.shape[1]))\n",
    "    sales_train_validation = reduce_mem_usage(sales_train_validation)\n",
    "    \n",
    "    sales_train_validation = sales_train_validation.iloc[-nrows:,:]  \n",
    "    \n",
    "    # seperate test dataframes\n",
    "    test1_rows = [row for row in submission['id'] if 'validation' in row]\n",
    "    test2_rows = [row for row in submission['id'] if 'evaluation' in row]\n",
    "    test1 = submission[submission['id'].isin(test1_rows)]\n",
    "    test2 = submission[submission['id'].isin(test2_rows)]\n",
    "    \n",
    "    # change column names\n",
    "    test1.columns = ['id', 'd_1914', 'd_1915', 'd_1916', 'd_1917', 'd_1918', 'd_1919', 'd_1920', 'd_1921', 'd_1922', 'd_1923', 'd_1924', 'd_1925', 'd_1926', 'd_1927', 'd_1928', 'd_1929', 'd_1930', 'd_1931', \n",
    "                      'd_1932', 'd_1933', 'd_1934', 'd_1935', 'd_1936', 'd_1937', 'd_1938', 'd_1939', 'd_1940', 'd_1941']\n",
    "    test2.columns = ['id', 'd_1942', 'd_1943', 'd_1944', 'd_1945', 'd_1946', 'd_1947', 'd_1948', 'd_1949', 'd_1950', 'd_1951', 'd_1952', 'd_1953', 'd_1954', 'd_1955', 'd_1956', 'd_1957', 'd_1958', 'd_1959', \n",
    "                      'd_1960', 'd_1961', 'd_1962', 'd_1963', 'd_1964', 'd_1965', 'd_1966', 'd_1967', 'd_1968', 'd_1969']\n",
    "    \n",
    "    \n",
    "    # get product table\n",
    "    product = sales_train_validation[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']].drop_duplicates()\n",
    "    \n",
    "    # merge with product table\n",
    "    test2['id'] = test2['id'].str.replace('_evaluation','_validation')\n",
    "    test1 = test1.merge(product, how = 'left', on = 'id')\n",
    "    test2 = test2.merge(product, how = 'left', on = 'id')\n",
    "    test2['id'] = test2['id'].str.replace('_validation','_evaluation')\n",
    "    \n",
    "    # \n",
    "    test1 = pd.melt(test1, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day', value_name = 'demand')\n",
    "    test2 = pd.melt(test2, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day', value_name = 'demand')\n",
    "    \n",
    "    \n",
    "    sales_train_validation['part'] = 'train'\n",
    "    test1['part'] = 'test1'\n",
    "    test2['part'] = 'test2'\n",
    "    \n",
    "    data = pd.concat([sales_train_validation, test1, test2], axis = 0)\n",
    "    \n",
    "    del sales_train_validation, test1, test2\n",
    "    \n",
    "    print(data.shape)\n",
    "    \n",
    "    # get only a sample for fst training\n",
    "#     data = data.loc[nrows:]\n",
    "    \n",
    "    # drop some calendar features\n",
    "    calendar.drop(['weekday', 'wday', 'month', 'year'], inplace = True, axis = 1)\n",
    "    \n",
    "    # delete test2 for now\n",
    "    data = data[data['part'] != 'test2']\n",
    "    \n",
    "    if merge:\n",
    "        # notebook crash with the entire dataset (maybee use tensorflow, dask, pyspark xD)\n",
    "        data = pd.merge(data, calendar, how = 'left', left_on = ['day'], right_on = ['d'])\n",
    "        data.drop(['day'], inplace = True, axis = 1)\n",
    "        # get the sell price data (this feature should be very important)\n",
    "        data = data.merge(sell_prices, on = ['store_id', 'item_id', 'wm_yr_wk'], how = 'left')\n",
    "        print('Our final dataset to train has {} rows and {} columns'.format(data.shape[0], data.shape[1]))\n",
    "    else: \n",
    "        pass\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melted sales train validation has 58327370 rows and 8 columns\n",
      "Memory usage of dataframe is 1002.74 MB\n",
      "Memory usage after optimization is: 669.08 MB\n",
      "Decreased by 33.3%\n",
      "(29207440, 9)\n",
      "Our final dataset to train has 28353720 rows and 19 columns\n"
     ]
    }
   ],
   "source": [
    "nrows = 27500000\n",
    "data = melt_and_merge(calendar_df, sell_prices_df, sales_train_validation_df, submission_df, nrows = nrows, merge = True)    \n",
    "data = data[5800861:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data):\n",
    "    nan_features = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
    "    for feature in nan_features:\n",
    "        data[feature].fillna('unknown', inplace = True)\n",
    "        \n",
    "    cat = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
    "    for feature in cat:\n",
    "        encoder = preprocessing.LabelEncoder()\n",
    "        data[feature] = encoder.fit_transform(data[feature])\n",
    "    return data\n",
    "\n",
    "def simple_fe(data):\n",
    "    \n",
    "    # rolling demand features\n",
    "    \n",
    "    for val in [28, 29, 30]:\n",
    "        data[f\"shift_t{val}\"] = data.groupby([\"id\"])[\"demand\"].transform(lambda x: x.shift(val))\n",
    "    for val in [7, 30, 60, 90, 180]:\n",
    "        data[f\"rolling_std_t{val}\"] = data.groupby([\"id\"])[\"demand\"].transform(lambda x: x.shift(28).rolling(val).std())\n",
    "    for val in [7, 30, 60, 90, 180]:\n",
    "        data[f\"rolling_mean_t{val}\"] = data.groupby([\"id\"])[\"demand\"].transform(lambda x: x.shift(28).rolling(val).mean())\n",
    "\n",
    "    data[\"rolling_skew_t30\"] = data.groupby([\"id\"])[\"demand\"].transform( lambda x: x.shift(28).rolling(30).skew())\n",
    "    data[\"rolling_kurt_t30\"] = data.groupby([\"id\"])[\"demand\"].transform(lambda x: x.shift(28).rolling(30).kurt())\n",
    "    \n",
    "        \n",
    "    # time features\n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    attrs = [\"year\", \"quarter\", \"month\", \"week\", \"day\", \"dayofweek\", \"is_year_end\", \"is_year_start\", \"is_quarter_end\", \\\n",
    "        \"is_quarter_start\", \"is_month_end\",\"is_month_start\",\n",
    "    ]\n",
    "\n",
    "    for attr in attrs:\n",
    "        dtype = np.int16 if attr == \"year\" else np.int8\n",
    "        data[attr] = getattr(data['date'].dt, attr).astype(dtype)\n",
    "    data[\"is_weekend\"] = data[\"dayofweek\"].isin([5, 6]).astype(np.int8)\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    return data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = simple_fe(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['id', 'd', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'demand',\n",
    "       'part', 'date', 'wm_yr_wk', 'snap_CA', 'snap_TX', 'snap_WI',\n",
    "       'sell_price', 'rolling_skew_t30', 'rolling_kurt_t30', 'year', 'quarter',\n",
    "       'month', 'week', 'dayofweek', 'is_year_end', 'is_year_start',\n",
    "       'is_quarter_end', 'is_quarter_start', 'is_month_end', 'is_month_start',\n",
    "       'is_weekend']\n",
    "\n",
    "data = data[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('final.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 3947.36 MB\n",
      "Memory usage after optimization is: 3199.88 MB\n",
      "Decreased by 18.9%\n"
     ]
    }
   ],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object and col_type.name != 'category' and 'datetime' not in col_type.name:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        elif 'datetime' not in col_type.name:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "   \n",
    "    return df\n",
    "\n",
    "train = reduce_mem_usage(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['rolling_skew_t30'].fillna(0, inplace=True)\n",
    "train['rolling_kurt_t30'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_pickle('train.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
