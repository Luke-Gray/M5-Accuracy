{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#General imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import re\n",
    "\n",
    "import os, sys, gc, time, warnings, pickle, psutil, random\n",
    "\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 'sales'         # Our main target\n",
    "END_TRAIN = 1913         # Last day in train set\n",
    "MAIN_INDEX = ['id','d']  # We can identify item by these columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('sales_train_validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ca = train[train['state_id']=='CA']#break up by state in order to compute faster\n",
    "train_tx = train[train['state_id']=='TX']\n",
    "train_wi = train[train['state_id']=='WI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_columns = ['id','item_id','dept_id','cat_id','store_id','state_id']\n",
    "train_ca = pd.melt(train_ca, \n",
    "                  id_vars = index_columns, \n",
    "                  var_name = 'd', \n",
    "                  value_name = TARGET)\n",
    "train_tx = pd.melt(train_tx, \n",
    "                  id_vars = index_columns, \n",
    "                  var_name = 'd', \n",
    "                  value_name = TARGET)\n",
    "train_wi = pd.melt(train_wi, \n",
    "                  id_vars = index_columns, \n",
    "                  var_name = 'd', \n",
    "                  value_name = TARGET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create some temporary df's for the purposes of engineering lag features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Given our size of our data, it might be wise to record how long each function takes so we know where\n",
    "our bottlenecks are.'''\n",
    "\n",
    "from functools import wraps\n",
    "\n",
    "def timer(func):\n",
    "    \"\"\"A decorator that prints how long a function took to run.\"\"\"\n",
    "\n",
    "    # Define the wrapper function to return.\n",
    "    @wraps #preserve the metadata of our function.\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # When wrapper() is called, get the current time.\n",
    "        t_start = time.time()\n",
    "\n",
    "        # Call the decorated function and store the result.\n",
    "        result = func(*args, **kwargs)\n",
    "\n",
    "        # Get the total time it took to run, and print it.\n",
    "        t_total = time.time() - t_start\n",
    "\n",
    "        print('{} took {}s'.format(func.__name__, t_total))        \n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.62 min: Time for bulk shift\n"
     ]
    }
   ],
   "source": [
    "'''Since our data is already sorted by 'd' values, we can easily shift() values\n",
    "as we as aggregate values.'''\n",
    "\n",
    "lags = [col for col in range(15,36,2)]\n",
    "\n",
    "start_time = time.time()\n",
    "ca_shifts = train_ca[['id','d',TARGET]]\n",
    "ca_shifts = ca_shifts.assign(**{\n",
    "        '{}_lag_{}'.format(col, l): ca_shifts.groupby(['id'])[col].transform(lambda x: x.shift(l))\n",
    "        for l in lags\n",
    "        for col in [TARGET]\n",
    "    })\n",
    "\n",
    "print('%0.2f min: Time for bulk shift' % ((time.time() - start_time) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_shifts = train_tx[['id','d',TARGET]]\n",
    "tx_shifts = tx_shifts.assign(**{\n",
    "        '{}_lag_{}'.format(col, l): tx_shifts.groupby(['id'])[col].transform(lambda x: x.shift(l))\n",
    "        for l in lags\n",
    "        for col in [TARGET]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wi_shifts = train_wi[['id','d',TARGET]]\n",
    "wi_shifts = wi_shifts.assign(**{\n",
    "        '{}_lag_{}'.format(col, l): wi_shifts.groupby(['id'])[col].transform(lambda x: x.shift(l))\n",
    "        for l in lags\n",
    "        for col in [TARGET]\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we generated lag features, we can focus on creating smoothing/rolling features. Afterwards, we can focus on dealing with the NaNs created from these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rolling period: 14\n",
      "Rolling period: 30\n",
      "Rolling period: 60\n",
      "Rolling period: 90\n",
      "Rolling period: 180\n"
     ]
    }
   ],
   "source": [
    "'''Rolling averages with different time frames'''\n",
    "\n",
    "ca_rolls = train_ca[['id','d','sales']]\n",
    "\n",
    "for i in [14,30,60, 90,180]:\n",
    "    print('Rolling period:', i)\n",
    "    ca_rolls['rolling_mean_'+str(i)] = ca_rolls.groupby(['id'])[TARGET].transform(lambda x: x.shift(1).rolling(i).mean())\n",
    "    ca_rolls['rolling_std_'+str(i)]  = ca_rolls.groupby(['id'])[TARGET].transform(lambda x: x.shift(1).rolling(i).std())\n",
    "    ca_rolls['rolling_max_'+str(i)]  = ca_rolls.groupby(['id'])[TARGET].transform(lambda x: x.shift(1).rolling(i).max())\n",
    "    ca_rolls['rolling_min_'+str(i)]  = ca_rolls.groupby(['id'])[TARGET].transform(lambda x: x.shift(1).rolling(i).min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rolling period: 14\n",
      "Rolling period: 30\n",
      "Rolling period: 60\n",
      "Rolling period: 90\n",
      "Rolling period: 180\n"
     ]
    }
   ],
   "source": [
    "'''Rolling averages with different time frames'''\n",
    "\n",
    "tx_rolls = train_tx[['id','d','sales']]\n",
    "\n",
    "for i in [14,30,60, 90,180]:\n",
    "    print('Rolling period:', i)\n",
    "    tx_rolls['rolling_mean_'+str(i)] = tx_rolls.groupby(['id'])[TARGET].transform(lambda x: x.shift(1).rolling(i).mean())\n",
    "    tx_rolls['rolling_std_'+str(i)]  = tx_rolls.groupby(['id'])[TARGET].transform(lambda x: x.shift(1).rolling(i).std())\n",
    "    tx_rolls['rolling_max_'+str(i)]  = tx_rolls.groupby(['id'])[TARGET].transform(lambda x: x.shift(1).rolling(i).max())\n",
    "    tx_rolls['rolling_min_'+str(i)]  = tx_rolls.groupby(['id'])[TARGET].transform(lambda x: x.shift(1).rolling(i).min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rolling period: 14\n",
      "Rolling period: 30\n",
      "Rolling period: 60\n",
      "Rolling period: 90\n",
      "Rolling period: 180\n"
     ]
    }
   ],
   "source": [
    "'''Rolling averages with different time frames'''\n",
    "\n",
    "wi_rolls = train_wi[['id','d','sales']]\n",
    "\n",
    "for i in [14,30,60, 90,180]:\n",
    "    print('Rolling period:', i)\n",
    "    wi_rolls['rolling_mean_'+str(i)] = wi_rolls.groupby(['id'])[TARGET].transform(lambda x: x.shift(1).rolling(i).mean())\n",
    "    wi_rolls['rolling_std_'+str(i)]  = wi_rolls.groupby(['id'])[TARGET].transform(lambda x: x.shift(1).rolling(i).std())\n",
    "    wi_rolls['rolling_max_'+str(i)]  = wi_rolls.groupby(['id'])[TARGET].transform(lambda x: x.shift(1).rolling(i).max())\n",
    "    wi_rolls['rolling_min_'+str(i)]  = wi_rolls.groupby(['id'])[TARGET].transform(lambda x: x.shift(1).rolling(i).min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'we can reduce memory by transforming our df to a sparse matrix and returning it back to a dense matrix. \\nLGB is able to handle ths type of matrix.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''we can reduce memory by transforming our df to a sparse matrix and returning it back to a dense matrix. \n",
    "LGB is able to handle ths type of matrix.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
