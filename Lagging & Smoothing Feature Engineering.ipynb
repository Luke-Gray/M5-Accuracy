{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#General imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import re\n",
    "\n",
    "import os, sys, gc, time, warnings, pickle, psutil, random\n",
    "\n",
    "from sklearn import preprocessing, metrics\n",
    "import gc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 'sales'         # Our main target\n",
    "END_TRAIN = 1941         # Last day in train set\n",
    "MAIN_INDEX = ['id','d']  # We can identify item by these columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ca = pd.read_pickle('ca.pkl')\n",
    "train_tx = pd.read_pickle('tx.pkl')\n",
    "train_wi = pd.read_pickle('wi.pkl')\n",
    "# short = pd.concat([ca,tx,wi])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create some temporary df's for the purposes of engineering lag features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Given our size of our data, it might be wise to record how long each function takes so we know where\n",
    "our bottlenecks are.'''\n",
    "\n",
    "from functools import wraps\n",
    "\n",
    "def timer(func):\n",
    "    \"\"\"A decorator that prints how long a function took to run.\"\"\"\n",
    "\n",
    "    # Define the wrapper function to return.\n",
    "    @wraps #preserve the metadata of our function.\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # When wrapper() is called, get the current time.\n",
    "        t_start = time.time()\n",
    "\n",
    "        # Call the decorated function and store the result.\n",
    "        result = func(*args, **kwargs)\n",
    "\n",
    "        # Get the total time it took to run, and print it.\n",
    "        t_total = time.time() - t_start\n",
    "\n",
    "        print('{} took {}s'.format(func.__name__, t_total))        \n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.86 min: Time for bulk shift\n"
     ]
    }
   ],
   "source": [
    "'''Since our data is already sorted by 'd' values, we can easily shift() values\n",
    "as we as aggregate values.'''\n",
    "\n",
    "lags = [col for col in range(17,32,3)]\n",
    "\n",
    "start_time = time.time()\n",
    "ca_shifts = train_ca[['id','date', 'd', TARGET]]\n",
    "ca_shifts = ca_shifts.assign(**{\n",
    "        '{}_lag_{}'.format('sales', l): ca_shifts.groupby(['id'])[col].transform(lambda x: x.shift(l))\n",
    "        for l in lags\n",
    "        for col in [TARGET]\n",
    "    })\n",
    "\n",
    "print('%0.2f min: Time for bulk shift' % ((time.time() - start_time) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_shifts = train_tx[['id','date', 'd', TARGET]]\n",
    "tx_shifts = tx_shifts.assign(**{\n",
    "        '{}_lag_{}'.format('sales', l): tx_shifts.groupby(['id'])[col].transform(lambda x: x.shift(l))\n",
    "        for l in lags\n",
    "        for col in [TARGET]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wi_shifts = train_wi[['id','date', 'd', TARGET]]\n",
    "wi_shifts = wi_shifts.assign(**{\n",
    "        '{}_lag_{}'.format('sales', l): wi_shifts.groupby(['id'])[col].transform(lambda x: x.shift(l))\n",
    "        for l in lags\n",
    "        for col in [TARGET]\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we generated lag features, we can focus on creating smoothing/rolling features. Afterwards, we can focus on dealing with the NaNs created from these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rolling period: 14\n",
      "Rolling period: 30\n",
      "Rolling period: 60\n",
      "Rolling period: 90\n",
      "Rolling period: 180\n"
     ]
    }
   ],
   "source": [
    "'''Rolling averages with different time frames'''\n",
    "\n",
    "ca_rolls = train_ca[['id','date', 'sales']]\n",
    "\n",
    "for i in [14,30,60, 90,180]:\n",
    "    print('Rolling period:', i)\n",
    "    ca_rolls['rolling_mean_'+str(i)] = ca_rolls.groupby(['id'])[TARGET].transform(lambda x: x.shift(1).rolling(i).mean())\n",
    "    ca_rolls['rolling_std_'+str(i)]  = ca_rolls.groupby(['id'])[TARGET].transform(lambda x: x.shift(1).rolling(i).std())\n",
    "    ca_rolls['rolling_max_'+str(i)]  = ca_rolls.groupby(['id'])[TARGET].transform(lambda x: x.shift(1).rolling(i).max())\n",
    "    ca_rolls['rolling_min_'+str(i)]  = ca_rolls.groupby(['id'])[TARGET].transform(lambda x: x.shift(1).rolling(i).min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rolling period: 14\n",
      "Rolling period: 30\n",
      "Rolling period: 60\n",
      "Rolling period: 90\n",
      "Rolling period: 180\n"
     ]
    }
   ],
   "source": [
    "'''Rolling averages with different time frames'''\n",
    "\n",
    "tx_rolls = train_tx[['id','date','sales']]\n",
    "\n",
    "for i in [14,30,60, 90,180]:\n",
    "    print('Rolling period:', i)\n",
    "    tx_rolls['rolling_mean_'+str(i)] = tx_rolls.groupby(['id'])[TARGET].transform(lambda x: x.shift(1).rolling(i).mean())\n",
    "    tx_rolls['rolling_std_'+str(i)]  = tx_rolls.groupby(['id'])[TARGET].transform(lambda x: x.shift(1).rolling(i).std())\n",
    "    tx_rolls['rolling_max_'+str(i)]  = tx_rolls.groupby(['id'])[TARGET].transform(lambda x: x.shift(1).rolling(i).max())\n",
    "    tx_rolls['rolling_min_'+str(i)]  = tx_rolls.groupby(['id'])[TARGET].transform(lambda x: x.shift(1).rolling(i).min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rolling period: 14\n",
      "Rolling period: 30\n",
      "Rolling period: 60\n",
      "Rolling period: 90\n",
      "Rolling period: 180\n"
     ]
    }
   ],
   "source": [
    "'''Rolling averages with different time frames'''\n",
    "\n",
    "wi_rolls = train_wi[['id','date','sales']]\n",
    "\n",
    "for i in [14,30,60, 90,180]:\n",
    "    print('Rolling period:', i)\n",
    "    wi_rolls['rolling_mean_'+str(i)] = wi_rolls.groupby(['id'])[TARGET].transform(lambda x: x.shift(1).rolling(i).mean())\n",
    "    wi_rolls['rolling_std_'+str(i)]  = wi_rolls.groupby(['id'])[TARGET].transform(lambda x: x.shift(1).rolling(i).std())\n",
    "    wi_rolls['rolling_max_'+str(i)]  = wi_rolls.groupby(['id'])[TARGET].transform(lambda x: x.shift(1).rolling(i).max())\n",
    "    wi_rolls['rolling_min_'+str(i)]  = wi_rolls.groupby(['id'])[TARGET].transform(lambda x: x.shift(1).rolling(i).min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca = ca_rolls.merge(ca_shifts, on=['id', 'date', 'sales'])\n",
    "tx = tx_rolls.merge(tx_shifts, on=['id', 'date', 'sales'])\n",
    "wi = wi_rolls.merge(wi_shifts, on=['id', 'date', 'sales'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal = pd.read_csv('calendar.csv', parse_dates = ['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_list=[i for i in cal.event_name_1.fillna(0).unique() if i != 0] \n",
    "\n",
    "#Extract all the days an event has in the span of 1916 days\n",
    "day_event_list=[cal[cal.event_name_1==i].d.tolist() for i in event_list]\n",
    "\n",
    "#Create the Event_df dataframe which we will use throughout the notebook\n",
    "event_df=pd.DataFrame({'Event Name' : event_list, 'Event day':day_event_list})\n",
    "restricted_day= set(['d_'+ str(i) for i in np.arange(1916,1970)])\n",
    "quantity=[]\n",
    "\n",
    "for i in day_event_list:\n",
    "    # Making sure that we exclude all the days thats are not in the training set\n",
    "    clean_i=list(set(i)-restricted_day)\n",
    "    temp=train[clean_i].sum().sum() #Adding columns and then rows\n",
    "    quantity.append(temp)\n",
    "\n",
    "event_df['Quantity']=quantity\n",
    "\n",
    "all_events = event_df['Event day'].values\n",
    "all_events = np.concatenate(all_events, axis=0)\n",
    "all_events = all_events.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'d'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2656\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2657\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2658\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'd'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-5a48cdb56251>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mca_rolls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'event'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mca_rolls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'd'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_events\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtx_rolls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'event'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtx_rolls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'd'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_events\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mwi_rolls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'event'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwi_rolls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'd'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_events\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2925\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2926\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2927\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2928\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2657\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2658\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2659\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2661\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'd'"
     ]
    }
   ],
   "source": [
    "ca_rolls['event'] = ca_rolls['d'].apply(lambda x: 1 if x in all_events else 0)\n",
    "tx_rolls['event'] = tx_rolls['d'].apply(lambda x: 1 if x in all_events else 0)\n",
    "wi_rolls['event'] = wi_rolls['d'].apply(lambda x: 1 if x in all_events else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's merge all of our datasets to get the prices of each item over time. This will allow is to do more complete feature engineering based on item prices, and change of price over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 208.77 MB\n",
      "Memory usage after optimization is: 45.77 MB\n",
      "Decreased by 78.1%\n",
      "Sell prices has 6841121 rows and 4 columns\n",
      "Memory usage of dataframe is 0.21 MB\n",
      "Memory usage after optimization is: 0.22 MB\n",
      "Decreased by -5.7%\n",
      "Calendar has 1969 rows and 14 columns\n",
      "Memory usage of dataframe is 452.91 MB\n",
      "Memory usage after optimization is: 96.55 MB\n",
      "Decreased by 78.7%\n",
      "Sales train validation has 30490 rows and 1947 columns\n"
     ]
    }
   ],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object and col_type.name != 'category' and 'datetime' not in col_type.name:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        elif 'datetime' not in col_type.name:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "   \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "sell_prices_df = pd.read_csv('sell_prices.csv')\n",
    "sell_prices_df = reduce_mem_usage(sell_prices_df)\n",
    "print('Sell prices has {} rows and {} columns'.format(sell_prices_df.shape[0], sell_prices_df.shape[1]))\n",
    "\n",
    "calendar_df = pd.read_csv('calendar.csv')\n",
    "calendar_df = reduce_mem_usage(calendar_df)\n",
    "print('Calendar has {} rows and {} columns'.format(calendar_df.shape[0], calendar_df.shape[1]))\n",
    "\n",
    "sales_train_validation_df = pd.read_csv('sales_train_evaluation.csv')\n",
    "sales_train_validation_df = reduce_mem_usage(sales_train_validation_df)\n",
    "print('Sales train validation has {} rows and {} columns'.format(sales_train_validation_df.shape[0], sales_train_validation_df.shape[1]))\n",
    "\n",
    "submission_df = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_ITEMS = sales_train_validation_df.shape[0]  # 30490\n",
    "DAYS_PRED = 28\n",
    "nrows = 365 * 2 * NUM_ITEMS\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 0.27 MB\n",
      "Memory usage after optimization is: 0.23 MB\n",
      "Decreased by 16.5%\n",
      "Memory usage of dataframe is 96.86 MB\n",
      "Memory usage after optimization is: 96.45 MB\n",
      "Decreased by 0.4%\n",
      "Memory usage of dataframe is 78.29 MB\n",
      "Memory usage after optimization is: 45.67 MB\n",
      "Decreased by 41.7%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def encode_categorical(df, cols):\n",
    "    for col in cols:\n",
    "        # Leave NaN as it is.\n",
    "        le = LabelEncoder()\n",
    "        not_null = df[col][df[col].notnull()]\n",
    "        df[col] = pd.Series(le.fit_transform(not_null), index=not_null.index)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "calendar_df = encode_categorical(calendar_df, [\"event_name_1\", \"event_type_1\", \"event_name_2\", \"event_type_2\"]).pipe(reduce_mem_usage)\n",
    "sales_train_validation_df = encode_categorical(sales_train_validation_df, [\"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]).pipe(reduce_mem_usage)\n",
    "sell_prices_df = encode_categorical(sell_prices_df, [\"item_id\", \"store_id\"]).pipe(reduce_mem_usage)    \n",
    "    \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def melt_and_merge(calendar, sell_prices, sales_train_validation, submission, nrows = 55000000, merge = False):\n",
    "    \n",
    "    # melt sales data, get it ready for training\n",
    "    sales_train_validation = pd.melt(sales_train_validation, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day', value_name = 'demand')\n",
    "    print('Melted sales train validation has {} rows and {} columns'.format(sales_train_validation.shape[0], sales_train_validation.shape[1]))\n",
    "    sales_train_validation = reduce_mem_usage(sales_train_validation)\n",
    "    \n",
    "    sales_train_validation = sales_train_validation.iloc[-nrows:,:]  \n",
    "    \n",
    "    # seperate test dataframes\n",
    "    test1_rows = [row for row in submission['id'] if 'validation' in row]\n",
    "    test2_rows = [row for row in submission['id'] if 'evaluation' in row]\n",
    "    test1 = submission[submission['id'].isin(test1_rows)]\n",
    "    test2 = submission[submission['id'].isin(test2_rows)]\n",
    "    \n",
    "    # change column names\n",
    "    test1.columns = ['id', 'd_1914', 'd_1915', 'd_1916', 'd_1917', 'd_1918', 'd_1919', 'd_1920', 'd_1921', 'd_1922', 'd_1923', 'd_1924', 'd_1925', 'd_1926', 'd_1927', 'd_1928', 'd_1929', 'd_1930', 'd_1931', \n",
    "                      'd_1932', 'd_1933', 'd_1934', 'd_1935', 'd_1936', 'd_1937', 'd_1938', 'd_1939', 'd_1940', 'd_1941']\n",
    "    test2.columns = ['id', 'd_1942', 'd_1943', 'd_1944', 'd_1945', 'd_1946', 'd_1947', 'd_1948', 'd_1949', 'd_1950', 'd_1951', 'd_1952', 'd_1953', 'd_1954', 'd_1955', 'd_1956', 'd_1957', 'd_1958', 'd_1959', \n",
    "                      'd_1960', 'd_1961', 'd_1962', 'd_1963', 'd_1964', 'd_1965', 'd_1966', 'd_1967', 'd_1968', 'd_1969']\n",
    "    \n",
    "    \n",
    "    # get product table\n",
    "    product = sales_train_validation[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']].drop_duplicates()\n",
    "    \n",
    "    # merge with product table\n",
    "    test2['id'] = test2['id'].str.replace('_evaluation','_validation')\n",
    "    test1 = test1.merge(product, how = 'left', on = 'id')\n",
    "    test2 = test2.merge(product, how = 'left', on = 'id')\n",
    "    test2['id'] = test2['id'].str.replace('_validation','_evaluation')\n",
    "    \n",
    "    # \n",
    "    test1 = pd.melt(test1, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day', value_name = 'demand')\n",
    "    test2 = pd.melt(test2, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day', value_name = 'demand')\n",
    "    \n",
    "    \n",
    "    sales_train_validation['part'] = 'train'\n",
    "    test1['part'] = 'test1'\n",
    "    test2['part'] = 'test2'\n",
    "    \n",
    "    data = pd.concat([sales_train_validation, test1, test2], axis = 0)\n",
    "    \n",
    "    del sales_train_validation, test1, test2\n",
    "    \n",
    "    print(data.shape)\n",
    "    \n",
    "    # get only a sample for fst training\n",
    "#     data = data.loc[nrows:]\n",
    "    \n",
    "    # drop some calendar features\n",
    "    calendar.drop(['weekday', 'wday', 'month', 'year'], inplace = True, axis = 1)\n",
    "    \n",
    "    # delete test2 for now\n",
    "    data = data[data['part'] != 'test2']\n",
    "    \n",
    "    if merge:\n",
    "        # notebook crash with the entire dataset (maybee use tensorflow, dask, pyspark xD)\n",
    "        data = pd.merge(data, calendar, how = 'left', left_on = ['day'], right_on = ['d'])\n",
    "        data.drop(['day'], inplace = True, axis = 1)\n",
    "        # get the sell price data (this feature should be very important)\n",
    "        data = data.merge(sell_prices, on = ['store_id', 'item_id', 'wm_yr_wk'], how = 'left')\n",
    "        print('Our final dataset to train has {} rows and {} columns'.format(data.shape[0], data.shape[1]))\n",
    "    else: \n",
    "        pass\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melted sales train validation has 59181090 rows and 8 columns\n",
      "Memory usage of dataframe is 1017.39 MB\n",
      "Memory usage after optimization is: 678.85 MB\n",
      "Decreased by 33.3%\n",
      "(29207440, 9)\n",
      "Our final dataset to train has 28353720 rows and 19 columns\n"
     ]
    }
   ],
   "source": [
    "nrows = 27500000\n",
    "data = melt_and_merge(calendar_df, sell_prices_df, sales_train_validation_df, submission_df, nrows = nrows, merge = True)    \n",
    "data = data[5800861:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data):\n",
    "    nan_features = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
    "    for feature in nan_features:\n",
    "        data[feature].fillna('unknown', inplace = True)\n",
    "        \n",
    "    cat = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
    "    for feature in cat:\n",
    "        encoder = preprocessing.LabelEncoder()\n",
    "        data[feature] = encoder.fit_transform(data[feature])\n",
    "    return data\n",
    "\n",
    "def simple_fe(data):\n",
    "    \n",
    "    # rolling demand features\n",
    "    \n",
    "    for val in [28, 29, 30]:\n",
    "        data[f\"shift_t{val}\"] = data.groupby([\"id\"])[\"demand\"].transform(lambda x: x.shift(val))\n",
    "    for val in [7, 30, 60, 90, 180]:\n",
    "        data[f\"rolling_std_t{val}\"] = data.groupby([\"id\"])[\"demand\"].transform(lambda x: x.shift(28).rolling(val).std())\n",
    "    for val in [7, 30, 60, 90, 180]:\n",
    "        data[f\"rolling_mean_t{val}\"] = data.groupby([\"id\"])[\"demand\"].transform(lambda x: x.shift(28).rolling(val).mean())\n",
    "\n",
    "    data[\"rolling_skew_t30\"] = data.groupby([\"id\"])[\"demand\"].transform( lambda x: x.shift(28).rolling(30).skew())\n",
    "    data[\"rolling_kurt_t30\"] = data.groupby([\"id\"])[\"demand\"].transform(lambda x: x.shift(28).rolling(30).kurt())\n",
    "    \n",
    "        \n",
    "    # time features\n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    attrs = [\"year\", \"quarter\", \"month\", \"week\", \"day\", \"dayofweek\", \"is_year_end\", \"is_year_start\", \"is_quarter_end\", \\\n",
    "        \"is_quarter_start\", \"is_month_end\",\"is_month_start\",\n",
    "    ]\n",
    "\n",
    "    for attr in attrs:\n",
    "        dtype = np.int16 if attr == \"year\" else np.int8\n",
    "        data[attr] = getattr(data['date'].dt, attr).astype(dtype)\n",
    "    data[\"is_weekend\"] = data[\"dayofweek\"].isin([5, 6]).astype(np.int8)\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    return data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = simple_fe(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['id', 'd', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'demand',\n",
    "       'part', 'date', 'wm_yr_wk', 'snap_CA', 'snap_TX', 'snap_WI',\n",
    "       'sell_price', 'year', 'quarter',\n",
    "       'month', 'dayofweek', 'is_year_end', 'is_year_start',\n",
    "       'is_weekend']\n",
    "\n",
    "data = data[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>d</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>demand</th>\n",
       "      <th>part</th>\n",
       "      <th>date</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>snap_CA</th>\n",
       "      <th>snap_TX</th>\n",
       "      <th>snap_WI</th>\n",
       "      <th>sell_price</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>month</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>is_year_end</th>\n",
       "      <th>is_year_start</th>\n",
       "      <th>is_weekend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5800861</th>\n",
       "      <td>HOUSEHOLD_1_030_CA_4_evaluation</td>\n",
       "      <td>d_1230</td>\n",
       "      <td>2031.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>2014-06-11</td>\n",
       "      <td>11419</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.968750</td>\n",
       "      <td>2014</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5800862</th>\n",
       "      <td>HOUSEHOLD_1_032_CA_4_evaluation</td>\n",
       "      <td>d_1230</td>\n",
       "      <td>2032.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>2014-06-11</td>\n",
       "      <td>11419</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10.859375</td>\n",
       "      <td>2014</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5800863</th>\n",
       "      <td>HOUSEHOLD_1_033_CA_4_evaluation</td>\n",
       "      <td>d_1230</td>\n",
       "      <td>2033.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>2014-06-11</td>\n",
       "      <td>11419</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.468750</td>\n",
       "      <td>2014</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5800864</th>\n",
       "      <td>HOUSEHOLD_1_034_CA_4_evaluation</td>\n",
       "      <td>d_1230</td>\n",
       "      <td>2034.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>2014-06-11</td>\n",
       "      <td>11419</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.970703</td>\n",
       "      <td>2014</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5800865</th>\n",
       "      <td>HOUSEHOLD_1_035_CA_4_evaluation</td>\n",
       "      <td>d_1230</td>\n",
       "      <td>2035.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>2014-06-11</td>\n",
       "      <td>11419</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.628906</td>\n",
       "      <td>2014</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      id       d  item_id  dept_id  cat_id  \\\n",
       "5800861  HOUSEHOLD_1_030_CA_4_evaluation  d_1230   2031.0      5.0     2.0   \n",
       "5800862  HOUSEHOLD_1_032_CA_4_evaluation  d_1230   2032.0      5.0     2.0   \n",
       "5800863  HOUSEHOLD_1_033_CA_4_evaluation  d_1230   2033.0      5.0     2.0   \n",
       "5800864  HOUSEHOLD_1_034_CA_4_evaluation  d_1230   2034.0      5.0     2.0   \n",
       "5800865  HOUSEHOLD_1_035_CA_4_evaluation  d_1230   2035.0      5.0     2.0   \n",
       "\n",
       "         store_id  state_id  demand   part       date  wm_yr_wk  snap_CA  \\\n",
       "5800861       3.0       0.0       1  train 2014-06-11     11419        0   \n",
       "5800862       3.0       0.0       0  train 2014-06-11     11419        0   \n",
       "5800863       3.0       0.0       0  train 2014-06-11     11419        0   \n",
       "5800864       3.0       0.0       0  train 2014-06-11     11419        0   \n",
       "5800865       3.0       0.0       0  train 2014-06-11     11419        0   \n",
       "\n",
       "         snap_TX  snap_WI  sell_price  year  quarter  month  dayofweek  \\\n",
       "5800861        1        1    4.968750  2014        2      6          2   \n",
       "5800862        1        1   10.859375  2014        2      6          2   \n",
       "5800863        1        1    4.468750  2014        2      6          2   \n",
       "5800864        1        1    3.970703  2014        2      6          2   \n",
       "5800865        1        1    4.628906  2014        2      6          2   \n",
       "\n",
       "         is_year_end  is_year_start  is_weekend  \n",
       "5800861            0              0           0  \n",
       "5800862            0              0           0  \n",
       "5800863            0              0           0  \n",
       "5800864            0              0           0  \n",
       "5800865            0              0           0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data merges with item prices, we can now  find trends in price changes and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The following numbers are grabbed from Statista.com. These number represent state populations per year, \n",
    "and state discretionary income per capita\n",
    "\n",
    "We will use this to find volatiliy parity'''\n",
    "\n",
    "ca_capita_2014 = 38600000\n",
    "ca_capita_2015 = 28900000\n",
    "ca_capita_2016 = 39200000\n",
    "ca_discretion = 57393\n",
    "ca_2014 = ca_discretion*ca_capita_2014\n",
    "ca_2015 = ca_discretion*ca_capita_2015\n",
    "ca_2016 = ca_discretion*ca_capita_2016\n",
    "\n",
    "tx_capita_2014 =27000000\n",
    "tx_capita_2015 =27500000\n",
    "tx_capita_2016 =2800000\n",
    "tx_discretion = 47821\n",
    "tx_2014 = tx_discretion*tx_capita_2014\n",
    "tx_2015 = tx_discretion*tx_capita_2015\n",
    "tx_2016 = tx_discretion*tx_capita_2016\n",
    "\n",
    "wi_capita_2014 = 5750000\n",
    "wi_capita_2015 = 5760000\n",
    "wi_capita_2016 = 5770000\n",
    "wi_discretion = 47515\n",
    "wi_2014 = wi_discretion*wi_capita_2014\n",
    "wi_2015 = wi_discretion*wi_capita_2015\n",
    "wi_2016 = wi_discretion*wi_capita_2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
