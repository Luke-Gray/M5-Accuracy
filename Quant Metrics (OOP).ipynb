{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import re\n",
    "import math\n",
    "from itertools import repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With minimal adjustment, this code black has been provided by many fellow M5 competitors. \n",
    "#And so, for that, I thank you!\n",
    "\n",
    "def get_sales_df(train_df):\n",
    "    TARGET     = 'sales'\n",
    "    END_TRAIN  = 1913        \n",
    "    MAIN_INDEX = ['id','d']\n",
    "\n",
    "    index_columns = ['id','item_id','dept_id','cat_id','store_id','state_id']\n",
    "    sales_df = pd.melt(train_df, \n",
    "                      id_vars    = index_columns, \n",
    "                      var_name   = 'd', \n",
    "                      value_name = TARGET)\n",
    "\n",
    "    add_grid = pd.DataFrame()\n",
    "    for i in range(1,29):\n",
    "        temp_df = train_df[index_columns]\n",
    "        temp_df = temp_df.drop_duplicates()\n",
    "        temp_df['d'] = 'd_'+ str(END_TRAIN+i)\n",
    "        temp_df[TARGET] = np.nan\n",
    "        add_grid = pd.concat([add_grid,temp_df])\n",
    "\n",
    "    sales_df = pd.concat([sales_df,add_grid])\n",
    "    sales_df = sales_df.reset_index(drop=True)\n",
    "\n",
    "    # Remove some temoprary DFs\n",
    "    del temp_df, add_grid, train_df\n",
    "\n",
    "    sales_df['cat'] = sales_df['id'].str.split('_').str.get(0)\n",
    "\n",
    "    for col in index_columns:\n",
    "        sales_df[col] = sales_df[col].astype('category')\n",
    "    return sales_df\n",
    "\n",
    "\n",
    "\n",
    "def get_events_df(cal):\n",
    "    event_list=[i for i in cal.event_name_1.fillna(0).unique() if i != 0] \n",
    "\n",
    "    #Extract all the days an event has in the span of 1916 days\n",
    "    day_event_list=[cal[cal.event_name_1==i].d.tolist() for i in event_list]\n",
    "\n",
    "    #Create the Event_df dataframe which we will use throughout the notebook\n",
    "    event_df=pd.DataFrame({'Event Name' : event_list, 'Event day':day_event_list})\n",
    "    restricted_day= set(['d_'+ str(i) for i in np.arange(1916,1970)])\n",
    "    quantity=[]\n",
    "\n",
    "    for i in day_event_list:\n",
    "        # Making sure that we exclude all the days thats are not in the training set\n",
    "        clean_i=list(set(i)-restricted_day)\n",
    "        temp=train_df[clean_i].sum().sum() #Adding columns and then rows\n",
    "        quantity.append(temp)\n",
    "\n",
    "    event_df['Quantity']=quantity\n",
    "\n",
    "    all_events = event_df['Event day'].values\n",
    "    all_events = np.concatenate(all_events, axis=0)\n",
    "    all_events = all_events.astype(str)\n",
    "    return all_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('sales_train_validation.csv')\n",
    "cal = pd.read_csv('calendar.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-c167dde38310>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msales_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_sales_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# all_events = get_events_df(cal)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-67e43ae9967d>\u001b[0m in \u001b[0;36mget_sales_df\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mtemp_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0msales_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cat'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msales_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindex_columns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\strings.py\u001b[0m in \u001b[0;36msplit\u001b[1;34m(self, pat, n, expand)\u001b[0m\n\u001b[0;32m   2390\u001b[0m         'method': 'split'})\n\u001b[0;32m   2391\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpand\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2392\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2393\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrap_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpand\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexpand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\strings.py\u001b[0m in \u001b[0;36mstr_split\u001b[1;34m(arr, pat, n)\u001b[0m\n\u001b[0;32m   1327\u001b[0m             \u001b[0mregex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1328\u001b[0m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mregex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxsplit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1329\u001b[1;33m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_na_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1330\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\strings.py\u001b[0m in \u001b[0;36m_na_map\u001b[1;34m(f, arr, na_result, dtype)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_na_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_result\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;31m# should really _check_ for NA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mna_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\strings.py\u001b[0m in \u001b[0;36m_map\u001b[1;34m(f, arr, na_mask, na_value, dtype)\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[0mconvert\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[1;31m# Reraise the exception if callable `f` got wrong number of args.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer_mask\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\strings.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1321\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m                 \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sales_df = get_sales_df(train_df)\n",
    "# all_events = get_events_df(cal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "QUANTITATIVE FINANCE AND TECHNICAL ANALYSIS METRICS TO APPLY TO EACH ITEM'S \n",
    "SALES. The different attributes are used to measure sale volatility, sale \n",
    "trend direction, sale trend direction, sale trend strength, sale averages \n",
    "of different periods, as well comparing these qualities to its benchmark \n",
    "(all items in the item's category.)\n",
    "'''\n",
    "\n",
    "class Quant_metrics():\n",
    "    '''\n",
    "    Quantitative finance metrics that can be utilized to measure sales \n",
    "    attributes of Walmart items.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, item):\n",
    "        self.item       = item\n",
    "        self.sales_df   = sales_df   \n",
    "        self.all_events = all_events\n",
    "        \n",
    "        self.food_benchmark      = self.sales_df.loc[self.sales_df['cat_id']=='FOODS']\n",
    "        self.hobbies_benchmark   = self.sales_df.loc[self.sales_df['cat_id']=='HOBBIES']\n",
    "        self.household_benchmark = self.sales_df.loc[self.sales_df['cat_id']=='HOUSEHOLD']\n",
    "    \n",
    "\n",
    "    \n",
    "    #2-STD BOLLINGER BANDS\n",
    "    def bollingers(self, window):\n",
    "        '''\n",
    "        2 standard deviation bollinger bands around the window-day rolling mean items sold per \n",
    "        day for specified item.\n",
    "\n",
    "        Args:\n",
    "            window(int)   : number of days we want to average over.\n",
    "        '''\n",
    "        item_df = pd.DataFrame(self.sales_df.loc[self.sales_df['id'] == self.item][['d','sales']])\n",
    "        item_df['mean'] = item_df['sales'].rolling(window=window).mean()\n",
    "        item_df['std'] = item_df['sales'].rolling(window=window).std()\n",
    "        item_df['upper_band'] = item_df['mean'] + (item_df['std'] * 2)\n",
    "        item_df['lower_band'] = item_df['mean'] - (item_df['std'] * 2)\n",
    "\n",
    "        #lets eliminate any outliers by finding values outside of the bands that are not on event days\n",
    "        item_df['sales'] = np.where(\n",
    "        (item_df['sales'] > item_df['upper_band']) & (item_df['d'].values not in self.all_events),\n",
    "        item_df['upper_band'], item_df['sales'])\n",
    "        item_df['sales'] = np.where(\n",
    "        (item_df['sales'] < item_df['lower_band']) & (item_df['d'].values not in self.all_events),\n",
    "        item_df['lower_band'], item_df['sales'])\n",
    "\n",
    "        return np.floor(item_df['sales'].values)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #RELATIVE STRENGTH USING EXPONENTIALLY-WEIGHTED MOVING AVERAGE\n",
    "    def RS(self, window, category=False):\n",
    "        '''\n",
    "        Relative strength of specified item compared to the average relative strength of \n",
    "        all items in its category.\n",
    "\n",
    "        Args:\n",
    "            window(int)   : number of days we want to average over.\n",
    "            category(bool): whether or not we are finding the RS of an item or a category.\n",
    "        '''\n",
    "\n",
    "        series = pd.DataFrame(self.sales_df.loc[self.sales_df['id'] == self.item]['sales'])\n",
    "\n",
    "        # Get the difference in price from previous step\n",
    "        delta = series.diff().dropna()\n",
    "        # Get rid of the first row, which is NaN since it did not have a previous \n",
    "        # row to calculate the differences\n",
    "        delta = delta[1:] \n",
    "\n",
    "        #Make the positive gains (up) and negative gains (down) Series\n",
    "        up, down = delta.copy(), delta.copy()\n",
    "        up[up < 0] = 0\n",
    "        down[down > 0] = 0\n",
    "\n",
    "        # Calculate the EWMA\n",
    "        roll_up1 = up.ewm(span=window).mean()\n",
    "        roll_down1 = down.abs().ewm(span=window).mean()\n",
    "\n",
    "        # Calculate the RSI based on EWMA\n",
    "        RS1 = roll_up1 / roll_down1\n",
    "        series['RS'] = RS1\n",
    "    #     RSI1 = 100.0 - (100.0 / (1.0 + RS1))\n",
    "\n",
    "        return series['RS']\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    #INVERSE FISHER TRANSFORM\n",
    "    def inverse_fisher(self, rsi_period, wma_period):\n",
    "        \"\"\"\n",
    "        Modified Inverse Fisher Transform applied on RSI.\n",
    "        Suggested method to use any IFT indicator is to buy when the indicator crosses over –0.5\n",
    "        or crosses over +0.5 if it has not previously crossed over –0.5 and to sell short when \n",
    "        the indicators crosses under +0.5 or crosses under –0.5 if it has not previously crossed\n",
    "        under +0.5.\n",
    "        \n",
    "        Args:\n",
    "            rsi_period(int) : Period over which we calculate the item's Relative Strength Index (RSI).\n",
    "            wma_period(int) : Period over which we calculate the item's weighted moving average (WMA).\n",
    "        \"\"\"\n",
    "        \n",
    "            #RELATIVE STRENGTH INDEX\n",
    "        def RSI2(period, adjust: bool = True):\n",
    "            \"\"\"\n",
    "            Relative Strength Index (RSI) is a momentum oscillator that measures the speed and \n",
    "            change of price movements. RSI oscillates between zero and 100. Traditionally, and\n",
    "            according to Wilder, RSI is considered overbought when above 70 and oversold when \n",
    "            below 30. Signals can also be generated by looking for divergences, failure swings \n",
    "            and centerline crossovers. RSI can also be used to identify the general trend.\n",
    "\n",
    "            Args:\n",
    "                period(int): Period over whcih we calculate the item's Relative Strength Index (RSI).\n",
    "            \"\"\"\n",
    "\n",
    "            ## get the price diff\n",
    "            delta = series.diff()\n",
    "\n",
    "            ## positive gains (up) and negative gains (down) Series\n",
    "            up, down = delta.copy(), delta.copy()\n",
    "            up[up < 0] = 0\n",
    "            down[down > 0] = 0\n",
    "\n",
    "            # EMAs of ups and downs\n",
    "            _gain = up.ewm(span=period, adjust=adjust).mean()\n",
    "            _loss = down.abs().ewm(span=period, adjust=adjust).mean()\n",
    "\n",
    "            RS = _gain / _loss\n",
    "            return pd.Series((100 - (100 / (1 + RS))))\n",
    "    \n",
    "        series = np.squeeze(self.sales_df.loc[self.sales_df['id'] == self.item]['sales'], axis=0)\n",
    "\n",
    "        v1 = pd.Series(0.1 * (RSI2(series, rsi_period) - 50), name=\"v1\")\n",
    "\n",
    "        ### v2 = WMA(wma_period) of v1\n",
    "        d = (wma_period * (wma_period + 1)) / 2  # denominator\n",
    "        rev = v1.iloc[::-1]  # reverse the series\n",
    "        wma = []\n",
    "\n",
    "        def _chunks(series, period):  # split into chunks of n elements\n",
    "            for i in enumerate(series):\n",
    "                c = rev.iloc[i[0] : i[0] + period]\n",
    "                if len(c) != period:\n",
    "                    yield None\n",
    "                else:\n",
    "                    yield c\n",
    "\n",
    "        def _wma(chunk, period):  # calculate wma for each chunk\n",
    "            w = []\n",
    "            for price, i in zip(chunk.iloc[::-1].items(), range(period + 1)[1:]):\n",
    "                w.append(price[1] * i / d)\n",
    "            return sum(w)\n",
    "\n",
    "        for i in _chunks(rev, self.wma_period):\n",
    "            try:\n",
    "                wma.append(_wma(i, self.wma_period))\n",
    "            except:\n",
    "                wma.append(None)\n",
    "\n",
    "        wma.reverse()  ##reverse the wma list t\n",
    "    #     ifish=(np.exp(2*v2)-1)/(np.exp(2*v2)+1)\n",
    "        v1[\"v2\"] = pd.Series(wma, index=v1.index)\n",
    "        fish = pd.Series(\n",
    "            ((2 * v1[\"v2\"]) - 1) ** 2 / ((2 * v1[\"v2\"]) + 1) ** 2, name=\"IFT_RSI\"\n",
    "        )\n",
    "        return fish\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    #RELATIVE ROTATION GRAPH\n",
    "    def RRG(self, window, steps):\n",
    "        '''Relative Rotation Graph (RRG) of a sample of items and its categories. RRGs are made \n",
    "        of an item's/category's relative strength and momentum.\n",
    "\n",
    "        Args:\n",
    "            window(int) : Moving average window of item's sales.\n",
    "            steps(int)  : number of days before next benchmark value is compared to item value. \n",
    "        '''\n",
    "\n",
    "        categ = self.sales_df.loc[self.sales_df['id']==self.item]['cat_id'].iloc[0]\n",
    "        \n",
    "        if categ == 'FOODS':\n",
    "            benchmark = self.food_benchmark\n",
    "        elif categ == 'HOBBIES':\n",
    "            benchmark = self.hobbies_benchmark\n",
    "        else:\n",
    "            benchmark = self.household_benchmark\n",
    "\n",
    "\n",
    "        # Get the difference in price from previous step\n",
    "        benchmark_delta = benchmark.diff().dropna()\n",
    "        # Get rid of the first row, which is NaN since it did not have a previous \n",
    "        # row to calculate the differences  \n",
    "        benchmark_delta = benchmark_delta[1:]\n",
    "\n",
    "        # Make the positive gains (up) and negative gains (down) Series\n",
    "        b_up, b_down = benchmark_delta.copy(), benchmark_delta.copy()\n",
    "        b_up[b_up < 0] = 0\n",
    "        b_down[b_down > 0] = 0\n",
    "\n",
    "        #Calculate EWMA for category series\n",
    "        b_roll_up2 = b_up.ewm(window).mean()\n",
    "        b_roll_down2 = b_down.abs().ewm(window).mean()  \n",
    "\n",
    "        b_RS2 = b_roll_up2 / b_roll_down2\n",
    "    #     b_RSI2 = 100.0 - (100.0 / (1.0 + b_RS2))\n",
    "        benchmark['jdk_rs'] = 100 + ((b_RS2 - b_RS2.mean()) / b_RS2.std() + 1)\n",
    "\n",
    "        b_mom = benchmark.iloc[:,0].diff(periods = window).fillna(0)\n",
    "        benchmark['jdk_mom'] = 100 + ((b_mom - b_mom.mean()) / b_mom.std() + 1)\n",
    "        benchmark.fillna(0, inplace=True)\n",
    "\n",
    "        #Repeat steps for the particular item we are indexing on\n",
    "        series = self.sales_df.loc[self.sales_df['id'] == self.item]['sales'].to_frame()\n",
    "\n",
    "        # Get the difference in price from previous step\n",
    "        item_delta = series.diff().dropna()\n",
    "\n",
    "        item_delta = item_delta[1:] \n",
    "\n",
    "        # Make the positive gains (up) and negative gains (down) Series\n",
    "        i_up, i_down = item_delta.copy(), item_delta.copy()\n",
    "        i_up[i_up < 0] = 0\n",
    "        i_down[i_down > 0] = 0\n",
    "\n",
    "        # Calculate the EWMA for item series\n",
    "        i_roll_up2 = i_up.ewm(window).mean()\n",
    "        i_roll_down2 = i_down.abs().ewm(window).mean()\n",
    "\n",
    "        # Calculate the RSI based on SMA\n",
    "        i_RS2 = i_roll_up2 / i_roll_down2\n",
    "        i_RS2.loc[i_RS2['sales']==np.inf, 'sales']=0\n",
    "    #     i_RSI2 = 100.0 - (100.0 / (1.0 + i_RS2))\n",
    "        series['jdk_rs'] = 100 + ((i_RS2 - i_RS2.mean()) / i_RS2.std() + 1)\n",
    "\n",
    "        i_mom = series.iloc[:,0].diff(periods = window).fillna(0)\n",
    "\n",
    "        series['jdk_mom'] = 100 + ((i_mom - i_mom.mean()) / i_mom.std() + 1)\n",
    "        series.fillna(0, inplace=True)\n",
    "\n",
    "        #create points to compare of steps-day intervals\n",
    "        ix = series.iloc[::steps]['jdk_rs'].values\n",
    "        iy = series.iloc[::steps][\"jdk_mom\"].values\n",
    "        item_vals = list(zip(ix,iy))\n",
    "\n",
    "        bx = benchmark.iloc[::steps, benchmark.columns.get_loc(\"jdk_rs\")].values\n",
    "        by = benchmark.iloc[::steps, benchmark.columns.get_loc(\"jdk_mom\")].values \n",
    "        bench_vals = list(zip(bx,by))\n",
    "\n",
    "\n",
    "        rrg_bench = []\n",
    "        rrg_item = []\n",
    "\n",
    "        for i in range(len(item_vals)):\n",
    "            if (item_vals[i][0]>bench_vals[i][0]) & (item_vals[i][1]>bench_vals[i][1]):\n",
    "                rrg_bench.extend(repeat(1,steps))\n",
    "            elif (item_vals[i][0]<bench_vals[i][0]) & (item_vals[i][1]>bench_vals[i][1]):\n",
    "                rrg_bench.extend(repeat(2,steps))\n",
    "            elif (item_vals[i][0]>bench_vals[i][0]) & (item_vals[i][1]<bench_vals[i][1]):\n",
    "                rrg_bench.extend(repeat(3,steps))\n",
    "            elif (item_vals[i][0]<bench_vals[i][0]) & (item_vals[i][1]<bench_vals[i][1]):\n",
    "                rrg_bench.extend(repeat(4,steps))\n",
    "            else:\n",
    "                rrg_bench.extend(repeat(5,steps))\n",
    "\n",
    "        for i in range(len(item_vals)):\n",
    "            if (item_vals[i][0]>item_vals[i-1][0]) & (item_vals[i][1]>item_vals[i-1][1]):\n",
    "                rrg_item.extend(repeat(1,steps))\n",
    "            elif (item_vals[i][0]<item_vals[i-1][0]) & (item_vals[i][1]>item_vals[i-1][1]):\n",
    "                rrg_item.extend(repeat(2,steps))\n",
    "            elif (item_vals[i][0]>item_vals[i-1][0]) & (item_vals[i][1]<item_vals[i-1][1]):\n",
    "                rrg_item.extend(repeat(3,steps))\n",
    "            elif (item_vals[i][0]<item_vals[i-1][0]) & (item_vals[i][1]<item_vals[i-1][1]):\n",
    "                rrg_item.extend(repeat(4,steps))\n",
    "            else:\n",
    "                rrg_item.extend(repeat(5,steps))\n",
    "\n",
    "        series['rrg_bench'] = np.asarray(rrg_bench)\n",
    "        series['rrg_item'] = np.asarray(rrg_item)\n",
    "\n",
    "        return series['rrg_bench'], series['rrg_item']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #AVERAGE TRUE RANGE\n",
    "    def ATR(self, window):\n",
    "        '''\n",
    "        Average True Range (ATR) is a metric used to measure the how much the data is trending.\n",
    "        When the ATR raises, that means the data is experiencing a trend in either direction. \n",
    "        Direction of trend is not identified with this metric.\n",
    "        \n",
    "        Args:\n",
    "            item(str)   : Walmart item we are evaluating.\n",
    "            window(int) : Moving average window of item's sales.\n",
    "        '''\n",
    "        \n",
    "        def wwma(values, window):\n",
    "            \"\"\"\n",
    "            J. Welles Wilder's exponentially weighted moving average.\n",
    "\n",
    "            Args:\n",
    "                window(int) : Moving average window of item's sales.\n",
    "            \"\"\"\n",
    "\n",
    "            return values.ewm(alpha=1/window, adjust=False).mean()\n",
    "        \n",
    "        item_df = pd.DataFrame({'sales':self.sales_df.loc[self.sales_df['id'] == self.item]['sales']})\n",
    "        item_wwma = item_df.ewm(alpha=1/window, adjust=False).mean() #Wilder's EMA\n",
    "\n",
    "        high = item_df.rolling(window).max()\n",
    "        low = item_df.rolling(window).min()\n",
    "        close = item_df.rolling(window).mean()\n",
    "        item_df['tr0'] = abs(high - low)\n",
    "        item_df['tr1'] = abs(high - close.shift())\n",
    "        item_df['tr2'] = abs(low - close.shift())\n",
    "        tr = item_df[['tr0', 'tr1', 'tr2']].max(axis=1)\n",
    "        atr = wwma(tr, window)\n",
    "        item_df['atr'] = atr\n",
    "        return item_df['atr'].fillna(method='ffill')\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #ANNUALIZED VOLATILITY\n",
    "    def annualized_volatility(self, window):\n",
    "        '''\n",
    "        Return the annualized standard deviation of daily log returns of item.\n",
    "\n",
    "        Args:\n",
    "            window(int) : Moving average window of item's sales.\n",
    "        '''\n",
    "\n",
    "\n",
    "        item_df = pd.DataFrame({'sales':self.sales_df.loc[short['id'] == self.item]['sales']})\n",
    "        item_df['ann_vol'] = item_df.diff().rolling(window).std()*(365**0.5)\n",
    "        return item_df['ann_vol']\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #ENTROPY\n",
    "    def calc_entropy(self):\n",
    "        \"\"\"\n",
    "        Calculate entropy given a pandas series, list, or numpy array.\n",
    "\n",
    "        Args:\n",
    "            window(int) : Moving average window of item's sales.\n",
    "        \"\"\"\n",
    "\n",
    "        series = pd.DataFrame({'sales':self.sales_df.loc[self.sales_df['id'] == self.item]['sales']})\n",
    "        series = np.squeeze(series)\n",
    "        # Compute the counts of each unique value in the column\n",
    "        counts = np.bincount(series)\n",
    "        # Divide by the total column length to get a probability\n",
    "        probabilities = counts / len(series)\n",
    "\n",
    "        # Initialize the entropy to 0\n",
    "        entropy = 0\n",
    "        # Loop through the probabilities, and add each one to the total entropy\n",
    "        for prob in probabilities:\n",
    "            if prob > 0:\n",
    "                entropy += prob * math.log(prob, 2)\n",
    "\n",
    "        return -entropy\n",
    "\n",
    "\n",
    "\n",
    "    #BETA\n",
    "    def beta(self, window):\n",
    "        '''\n",
    "        Beta compares the rolling sales of the specified item compared to its \n",
    "        benchmark.\n",
    "        \n",
    "        Args:\n",
    "            window(int) : Moving average window of item's sales.\n",
    "        '''\n",
    "\n",
    "        categ = self.sales_df.loc[self.sales_df['id']==self.item]['cat_id'].iloc[0]\n",
    "        if categ   == 'FOODS':\n",
    "            benchmark = food_benchmark.iloc[:,0]\n",
    "        elif categ == 'HOBBIES':\n",
    "            benchmark = hobbies_benchmark.iloc[:,0]\n",
    "        else:\n",
    "            benchmark = household_benchmark.iloc[:,0]\n",
    "\n",
    "        series = self.sales_df.loc[self.sales_df['id'] == self.item]['sales'].to_frame()\n",
    "        benchmark = benchmark[:len(series)]\n",
    "\n",
    "        series_roll = np.squeeze(series.rolling(window).std())\n",
    "        bench_roll = benchmark.rolling(window).std()\n",
    "        series_roll.fillna(0, inplace=True)\n",
    "        bench_roll.fillna(0,inplace=True)\n",
    "\n",
    "        beta = series_roll.values/bench_roll.values\n",
    "\n",
    "        series['beta'] = beta\n",
    "        series['beta'].fillna(0, inplace=True)\n",
    "        return series['beta']\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    #INFORMATION RATIO\n",
    "    def information_ratio(self, window):\n",
    "        '''\n",
    "        Information ratio measures item sales returns over its standard deviation of returns.\n",
    "        \n",
    "        Args:\n",
    "            window(int) : Moving average window of item's sales.\n",
    "        '''    \n",
    "\n",
    "        categ = self.sales_df.loc[self.sales_df['id']==self.item]['cat_id'].iloc[0]\n",
    "        if categ == 'FOODS':\n",
    "            benchmark = food_benchmark.iloc[:,0]\n",
    "        elif categ == 'HOBBIES':\n",
    "            benchmark = hobbies_benchmark.iloc[:,0]\n",
    "        else:\n",
    "            benchmark = household_benchmark.iloc[:,0]\n",
    "\n",
    "        series = self.sales_df.loc[self.sales_df['id'] == self.item]['sales'].to_frame()\n",
    "        benchmark = benchmark[:len(series)]\n",
    "\n",
    "        return_difference = series.values - benchmark.values\n",
    "        volatility = return_difference.std() * np.sqrt(window) \n",
    "        information_ratio = return_difference.mean() / volatility\n",
    "\n",
    "        series['info_ratio'] = information_ratio\n",
    "\n",
    "        return series['info_ratio']\n",
    "\n",
    "    \n",
    "    \n",
    "    #TRIPLE EXPONENTIALLY-WEIGHTED MOVING AVERAGE\n",
    "    def TEMA(self, window, adjust = True):\n",
    "        \"\"\"\n",
    "        Triple exponentially moving average attempts to remove the inherent lag associated\n",
    "        to Moving Averages by placing more weight on recent values. The name suggests this\n",
    "        is achieved by applying a triple exponential smoothing which is not the case. The \n",
    "        name triple comes from the fact that the value of an EMA (Exponential Moving Average)\n",
    "        is triple. To keep it in line with the actual data and to remove the lag the value \n",
    "        'EMA of EMA' is subtracted 3 times from the previously tripled EMA. Finally 'EMA of \n",
    "        EMA of EMA' is added. Because needed by a regular EMA.\n",
    "        \n",
    "        Args:\n",
    "            window(int) : Moving average window of item's sales.\n",
    "        \"\"\"\n",
    "        \n",
    "            #EXPONENTIALLY-WEIGHTED MOVING AVERGAE\n",
    "        def EMA(window, adjust = True):\n",
    "            \"\"\"\n",
    "            The exponentially weighted moving average, Like all moving average indicators, \n",
    "            they are much better suited for trending markets. When the market is in a strong\n",
    "            and sustained uptrend, the EMA indicator line will also show an uptrend and \n",
    "            vice-versa for a down trend. EMAs are commonly used in conjunction with other \n",
    "            indicators to confirm significant market moves and to gauge their validity.\n",
    "\n",
    "            Args:\n",
    "                window(int) : Moving average window of item's sales.\n",
    "            \"\"\"\n",
    "\n",
    "            item_df = self.sales_df.loc[self.sales_df['id'] == item]['sales'].to_frame()\n",
    "            return pd.Series(\n",
    "                item_df.iloc[:,0]\n",
    "                .ewm(span=window, adjust=adjust)\n",
    "                .mean(),\n",
    "                name=\"{0} period EMA\".format(window),\n",
    "            )\n",
    "        \n",
    "        item_df = self.sales_df.loc[self.sales_df['id'] == self.item]['sales'].to_frame()\n",
    "        triple_ema = 3 * EMA(window)\n",
    "        ema_ema_ema = (\n",
    "            EMA(window)\n",
    "            .ewm(ignore_na=False, span=window, adjust=adjust)\n",
    "            .mean()\n",
    "            .ewm(ignore_na=False, span=window, adjust=adjust)\n",
    "            .mean()\n",
    "        )\n",
    "\n",
    "        TEMA = (\n",
    "            triple_ema\n",
    "            - 3\n",
    "            * EMA(window)\n",
    "            .ewm(span=window, adjust=adjust)\n",
    "            .mean()\n",
    "            + ema_ema_ema\n",
    "        )\n",
    "\n",
    "        return pd.Series(TEMA, name=\"{0} period TEMA\".format(window))\n",
    "    \n",
    "    \n",
    "    \n",
    "    #RELATIVE VOLATILITY\n",
    "    def relative_volatility(self, w1, w2):\n",
    "        '''\n",
    "        The relative volatlity divides an item's sales of by a short-period moving average \n",
    "        by the item's sales over a loner period, shifted over on another.\n",
    "        \n",
    "        Args:\n",
    "            w1(int)  : shorter-period moving average window of item's sales.\n",
    "            w2(int)  : longer-period moving average window of item's sales.\n",
    "        '''\n",
    "        \n",
    "        series = self.sales_df.loc[self.sales_df['id'] == self.item]['sales'].to_frame()\n",
    "        r1 = series.rolling(w1).std()\n",
    "        r2 = series.rolling(w2).std()\n",
    "        r1 = r1.shift(w2)\n",
    "        r2 = r2.shift(-w1)\n",
    "\n",
    "        rv =r1/r2\n",
    "        series['rv'] = rv\n",
    "        return series['rv']\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #SEMIVARIANCE\n",
    "    def semivariance(self, w1, w2):\n",
    "    '''\n",
    "    Semivariance records the variance of sales that fall below the sales average of a period of time.\n",
    "    \n",
    "    Args:\n",
    "        w1(int)  : shorter-period moving average window of item's sales.\n",
    "        w2(int)  : longer-period moving average window of item's sales.\n",
    "    '''\n",
    "    \n",
    "    series = self.sales_df.loc[(self.sales_df['id']==self.item) & (self.sales_df['sales'].notna())]['sales'].to_frame()\n",
    "    series['mean'] = series['sales'].rolling(w1).mean().fillna(series['sales'].mean())\n",
    "    series.loc[series['sales']<series['mean'], 'semivariance'] = series['sales'].rolling(w2).var()\n",
    "    \n",
    "    return series['semivariance']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
